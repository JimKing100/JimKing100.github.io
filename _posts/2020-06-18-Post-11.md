---
layout: post
title: Using a Neural Network to Predict Fantasy Football Points
subtitle: A Fantasy Football Trade Analyzer Using RNN-LSTM, ARIMA, XGBoost and Dash
image: '/img/nfl/football.gif'
comments: true
published: false
---

While the complete code for predicting fantasy football points is quite involved, let's take a closer look at the core RNN-LSTM code using one player as an example.  We are going to use Aaron Rodgers, the quarterback for the Green Bay Packers.  Aaron is an excellent test case as he started his NFL career in 2005 providing 15 years of data.  In addition, he did not suffer any significant injuries during the 2019 season.

The actual points for each 2019 NFL player is stored in the **original_df** dataframe.  The first five columns of the dataframe contain descriptive features for each player.  The next 320 columns contain the actual fantasy football points for each game from 2000 to 2019 for each player (16 games per season x 20 seasons = 320 data points).  These values are NaN until the player's first NFL season.  So Aaron Rodgers actual data does not begin until column 85 (16 x 5 = 80 games until the 2005 season plus the 5 descriptive columns).

![original_df DataFrame](/img/nfl/original_df.png)

The initial call to the RNN-LSTM prediction function **lstm_pred** occurs in the function named **main**.  The input to the function consists of:

**player** = 'Aaron Rodgers' (This is the name of the player)  
**n_periods** = 16  (In this example we are beginning at the start of the season and predicting all 16 games of the 2019 season)  
**col** = 85 (Aaron's 2005 data begins in the 85th column of the dataframe)  

```
pred_points = lstm_pred(player, n_periods, col)
```
Using these three inputs, let's take a close look at **lstm_pred**.

```
def lstm_pred(p, np, c):
    series = make_series(p, c)
    X = series.values
    supervised = timeseries_to_supervised(X, 1)
    supervised_values = supervised.values

    # Split data into train and test-sets
    train, test = supervised_values[0:-np], supervised_values[-np:]

    # Transform the scale of the data
    scaler, train_scaled, test_scaled = scale(train, test)

    # Fit the model
    lstm_model = fit_lstm(train_scaled, 1, 100, 1)

    # Forecast the entire training dataset to build up state for forecasting
    train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)
    lstm_model.predict(train_reshaped, batch_size=1)

    # Walk-forward validation on the test data
    yhat_sum = 0
    for i in range(len(test_scaled)):
        # Make one-step forecast
        X, y = test_scaled[i, 0:-1], test_scaled[i, -1]
        yhat = forecast_lstm(lstm_model, 1, X)
        # Invert scaling
        yhat = invert_scale(scaler, X, yhat)
        # Sum the weekly forecasts
        yhat_sum = yhat_sum + yhat

    return yhat_sum

```

Let's walk through this code to better understand how it works.  The first four lines are simply preparing the data for the model.

```
series = make_series(p, c)
X = series.values
supervised = timeseries_to_supervised(X, 1)
supervised_values = supervised.values
```

The function **make_series** simply converts Aaron's actual points stored in the dataframe from 2005 through 2019 into a python Series.  The values of the series are then stored in **X**.  The **X** values now represents our time series for Aaron's fantasy football points.  The function **timeseries_to_supervised(X, 1)** takes the time series **X** and creates a dataframe containing **X** our supervised learning input pattern and **y** our supervised learning output pattern.  The **y** values are simply the **X** series shifted back one period.  The **X** and **y** values are then stored in the numpy array **supervised_values**.  What we've done with these four lines of code (and their functions) is taken Aaron's fantasy football point production from the original_df dataframe and converted it into a form for supervised learning. The tail end of the **supervised_values** representing the 2019 season is below.  The first column represents the **X** values and the second column the **y** values.

```
 ....
 [ 1.04 12.92]
 [12.92 14.36]
 [14.36 14.3 ]
 [14.3  25.48]
 [25.48  9.42]
 [ 9.42 18.32]
 [18.32 44.76]
 [44.76 28.1 ]
 [28.1  12.94]
 [12.94 10.02]
 [10.02  9.46]
 [ 9.46 28.12]
 [28.12 11.4 ]
 [11.4  14.42]
 [14.42  9.34]
 [ 9.34 19.02]]
```
The next line of code splits the **supervised_values** into **train** and **test** splitting off the last 16 games (the 2019 season) into the test set and leaving the 2005 - 2018 seasons as the training set.

```
train, test = supervised_values[0:-np], supervised_values[-np:]
```

Once the code is split, the data is normalized using the **scale** function.  The scale function uses the MinMaxScaler with a range of (-1,1).  Normalizing the data is recommended as it should make learning easier for a neural network and should ensure the magnitude of the values are more or less similar.

```
scaler, train_scaled, test_scaled = scale(train, test)
```

We are now ready to fit the RNN-LSTM model passing the function **fit_lstm** the following parameters:  
**train** - train_scaled  
**batch_size** - 1  
**nb_epochs** - 100  
**neurons** - 1  

```
lstm_model = fit_lstm(train_scaled, 1, 100, 1)
```

The **fit_lstm** function is where the RNN-LSTM magic happens.  The actual mechanics of the function are quite involved and for those wishing to obtain further information please see the detailed description of the function found in the LSTM Model Development section of the article *[Time Series Forecasting with the Long Short-Term Memory Network in Python by Jason Brownlee][1]* whose code I borrowed for this function.

The article recommends a higher number of epochs (1000 - 4000) and neurons (1-4) for better results, but due to time and resource requirements of running forecasts for several hundered players, the 100 epochs and 1 neuron provided very good results in a less resource intensive manner.

We are now ready to make forecasts using the **lstm_model.predict** function of the model.

```
train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)
lstm_model.predict(train_reshaped, batch_size=1)

```
The function requires a 3D numpy array as input and in our case this will be an array of one value (the observation of the previous time step) and outputs a 2D numpy array of one value.  Before we forecast the test data we need to seed the initial state by making a prediction of all the values in the training data.  The first line of the code above reshapes the data into the one value 3D numpy array and the second line sets the initial state for the network using the training data.

All we have left is to step through the test data making individual forecasts for the 16 games and summing those values to create the predicted fantasy football points for Aaron's season.

```
yhat_sum = 0
for i in range(len(test_scaled)):
    # Make one-step forecast
    X, y = test_scaled[i, 0:-1], test_scaled[i, -1]
    yhat = forecast_lstm(lstm_model, 1, X)
    # Invert scaling
    yhat = invert_scale(scaler, X, yhat)
    # Sum the weekly forecasts
    yhat_sum = yhat_sum + yhat
```

```
yhat =  13.854151899814605
yhat =  15.803713732957839
yhat =  17.423491576910017
yhat =  18.172971495985983
yhat =  20.452734234929082
yhat =  18.303702155351637
yhat =  19.934223534464834
yhat =  18.4641663479805
yhat =  20.972200154066083
yhat =  20.11708943128586
yhat =  18.25905997931957
yhat =  16.523207017183303
yhat =  19.89731903553009
yhat =  18.599731026291845
yhat =  18.957313760519025
yhat =  16.89258553981781
```


[1]: <https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/>
