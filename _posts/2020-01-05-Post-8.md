---
layout: post
title: How to Use NLP to Find a Tech Job and Win a Hackathon
subtitle: Using Web Scaping, NLP and Flask to Create a Tech Job Search Web App  
image: '/img/Marin.jpg'
comments: true
published: false
---

I was looking for an interesting project for a recent Hackathon and decided to create a Web app that scraped job listings from Indeed.com, processed them using Natural Language Processing (NLP) and provided a summary of the total number of jobs, the average salary range and the top 10 technical skills for four broad tech positions in the top 20 tech cities in the U.S.

The four broad tech positions – Data Scientist, Web Developer, UX Designer and iOS Developer – actually include all positions related to that job title. So, a Web Developer includes Web Developer, Front End Engineer, PHP Developer and many other titles deemed by Indeed.com to be Web Developer-related positions.

The top 20 tech cities were determined by calculating the top 20 metro areas(MSAs) in the U.S. based on the highest average mean wage for computer and mathematical occupations. The Indeed.com search takes a 25 mile radius from the city, so the results include surrounding areas of the cities.

First, take a look at the finished product:

<iframe src="https://techjobsearch.herokuapp.com" width="900" height="900" style="border: none;"></iframe>


While the Web app is very simple to use, it required significant data science behind the scenes to construct the summary results.  Let's step through the process of Web scraping, NLP and app construction needed to bring the site to life in the 30 hour time limit of the Hackathon.

### A Word About the Code

All the code, data and associated files for the project can be accessed at my [GitHub][1].  The README file provides details of the repo directory and files.  The Web scraping and NLP were run using Jupyter Notebook. 

### Web Scraping

The Web scraping code uses [BeautifulSoup][2] to locate and scrape the data from the Indeed.com site (as a side note, always check the robots.txt file before scraping).  The initial data we are scraping:

1) Job Title
2) Company
3) Location
4) Salary
5) Job Description
6) Job Count

Let's take a snippet of code to demonstrate the basic process.

```
def extract_job_title(soup):
    jobs = []
    for div in soup.find_all(name="div", attrs={"class":"row"}):
        for a in div.find_all(name="a", attrs={"data-tn-element":"jobTitle"}):
            jobs.append(a["title"])
    return(jobs)

city_url = "https://www.indeed.com/jobs?q=" + title_name + \
           "&l=" + city_name + "%2C+" + st_name + \
           "&start=" + str(start)
page = requests.get(city_url)
soup = BeautifulSoup(page.text, "html.parser")

job_title_list.extend(extract_job_title(soup))
      
```

In the above code **city_url** is constructed using the job title, city and state.  The html page is then retrieved and stored in the **page** variable.  BeautifulSoup then parses the page into its html components and stores them in the Beautiful soup object **soup** (a nested data structure).

The **extract_job_title** function is then called passing it the soup object.  The **soup.find_all** method locates the approriate html and navigates to unique html components associated with that value.  In this case the Job Title is found as the subcomponent \<a data-tn-element="jobTitle"\> in \<div class="row"\>.  Your browser inspector becomes an essential tool when Web scraping!

The scraping is time consuming.  It takes about 1.5 hours to scrape about 100 jobs from the 80 permutations of 4 job titles and 20 cities.  For this reason, the Web app is not live and needs the data to be preprocessed and stored in some form.  The resulting dataframe created from the scraping is therefore downloaded to both a .csv file and a sqlite DB table.  The .csv file was ultimately used for it's ease and speed.  

The full code for the scraper, scraper.ipynb, can be found [here][3].

[1]: <https://github.com/JimKing100/techsearch>
[2]: <https://www.crummy.com/software/BeautifulSoup/bs4/doc/>
[3]: <https://github.com/JimKing100/techsearch/blob/master/preprocess/scraper.ipynb>

