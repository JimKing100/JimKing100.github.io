---
layout: post
title: How to Implement a Decision Tree Classifier in Python
subtitle: Create a Decision Tree Classifier from Scratch
image: '/img/dtree/rectangle_dtree.png'
comments: true
published: false
---

A decision tree is a popular and powerful method for making predictions in data science.  Decision trees also form the foundation for other popular ensemble methods such as bagging, boosting and gradient boosting.  Its popularity is due to the simplicity of the technique making it easy to understand.  We are going to implement a decision tree from scratch and use it for several classification problems.  First, lets start with a simple classification example to explain how a decision tree works.

### A Simple Example

Let's say we have 10 rectangles of various widths and heights.  Five of the rectangles are purple and five are yellow.  The data is shown below with X1 representing the width, X2 representing the height and Y representing the classes of 0 for purple rectangles and 1 for yellow rectangles:

![Rectangle Data](/img/dtree/rectangle_data.png)

Graphing the rectangles we can very clearly see the separate classes.

![Rectangle Graph](/img/dtree/rectangle_graph.png)

Based on the rectangle data, we can build a simple decision tree to make forecasts.  Decision trees are made up of decision nodes and leaf nodes.  In the decision tree below we start with the top-most box which represents the root of the tree (a decision node).  The first line of text in the root depicts the optimal initial decision of splitting the tree based on the width (X1) being less than 5.3.  The second line represents the initial Gini score which we will go into more detail later.  The third line represents the number of samples at this initial level - in this case 10.  The fourth line represents the number of items in each class for the node  - 5 for purple rectangles and 5 for yellow rectangles.

![Rectangle Decision Tree](/img/dtree/rectangle_dtree.png)

After splitting the data by width (X1) less than 5.3 we get two leaf nodes with 5 items in each node.  All the purple rectangles (0) are in one leaf node and all the yellow rectangles (1) are in the other leaf node.  Their corresponding Gini score, sample size and values are updated to reflect the split.

In this very simple example, we can predict whether a given rectangle is purple or yellow by simply checking if the width of the rectangle is less than 5.3.

### The Gini Index

The key to building a decision tree is determining the optimal split at each decision node.  Using the simple example above, how did we know to split the root at a width (X1) of 5.3?  The answer lies with the Gini index or score.  The Gini index is a cost function used to evaluate splits.  It is defined as follows:

The sum of p(1-p) over all classes, with p the frequencey of a class within a node.  Since the sum of p is 1, the formula can be represented as 1 - sum(p squared).  The Gini index calculates the amount of probability of a specific feature that is classified incorrectly when randomly selected and varies between 0 and .5.

Using our simple 2 class example, if we have 100% purple rectangles the Gini index is 0 or if we have 100% yellow rectangles the Gini index is also 0 - there is no incorrect classification, the data is pure (1 - ((1.0 * 1.0) + (0.0 * 0.0))).  However, if we have an 80/20 mix of rectangles (80 purple and 20 yellow or 80 yellow and 20 purple), then our Gini index is .32 (1 - ((.8 * .8) + (.2 * .2)).  A 50% split results in a Gini index of .5 (1 - ((.5 * .5) + (.5 * .5))) - an equal distribution of rectangles in the 2 classes.






