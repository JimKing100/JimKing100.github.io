---
layout: post
title: How to Implement a Decision Tree Classifier in Python
subtitle: Create a Decision Tree Classifier from Scratch
image: '/img/dtree/rectangle_dtree.png'
comments: true
published: false
---

A decision tree is a popular and powerful method for making predictions in data science.  Decision trees also form the foundation for other popular ensemble methods such as bagging, boosting and gradient boosting.  Its popularity is due to the simplicity of the technique making it easy to understand.  We are going to implement a decision tree from scratch and use it for several classification problems.  First, lets start with a simple classification example to explain how a decision tree works.

### A Simple Example

Let's say we have 10 rectangles of various widths and heights.  Five of the rectangles are purple and five are yellow.  The data is shown below with X1 representing the width, X2 representing the height and Y representing the classes of 0 for purple rectangles and 1 for yellow rectangles:

![Rectangle Data](/img/dtree/rectangle_data.png)

Graphing the rectangles we can very clearly see the separate classes.

![Rectangle Graph](/img/dtree/rectangle_graph.png)

Based on the rectangle data, we can build a simple decision tree to make forecasts.  Decision trees are made up of decision nodes and leaf nodes.  In the decision tree below we start with the top-most box which represents the root of the tree (a decision node).  The first line of text in the root depicts the optimal initial decision of splitting the tree based on the width (X1) being less than 5.3.  The second line represents the initial Gini score which we will go into more detail later.  The third line represents the number of samples at this initial level - in this case 10.  The fourth line represents the number of items in each class for the node  - 5 for purple rectangles and 5 for yellow rectangles.

![Rectangle Decision Tree](/img/dtree/rectangle_dtree.png)

After splitting the data by width (X1) less than 5.3 we get two leaf nodes with 5 items in each node.  All the purple rectangles (0) are in one leaf node and all the yellow rectangles (1) are in the other leaf node.  Their corresponding Gini score, sample size and values are updated to reflect the split.

In this very simple example, we can predict whether a given rectangle is purple or yellow by simply checking if the width of the rectangle is less than 5.3.

### The Gini Index

The key to building a decision tree is determining the optimal split at each decision node.  Using the simple example above, how did we know to split the root at a width (X1) of 5.3?  The answer lies with the Gini index or score.  The Gini index is a cost function used to evaluate splits.  It is defined as follows:

The sum of p(1-p) over all classes, with p the proportion of a class within a node.  Since the sum of p is 1, the formula can be represented as 1 - sum(p squared).  The Gini index calculates the amount of probability of a specific feature that is classified incorrectly when randomly selected and varies between 0 and .5.

Using our simple 2 class example, the Gini index for the root node is (1 - ((.5 * .5) + (.5 * .5))) = .5 - an equal distribution of rectangles in the 2 classes.  So 50% of our dataset at this node is classified incorrectly.  If the Gini score were 0, then 100% of our dataset at this node would be classified correctly (0% incorrect).  Our goal then is to use the lowest Gini score to build the decision tree.

### Determining the Best Split

In order to determine the best split, we need to iterate through all the features and consider the midpoints between adjacent training samples as a candidate split.  We then need to evaluate the cost of the split (Gini) and find the optimal split (lowest Gini).

Let's run through one example of calculating the Gini for one feature:

1) Sorting X1 in ascending order we get the first value of 1.72857131
2) For class 0, the split is 1 to the left and 4 to the right (one item <= 1.72857131, four items > 1.72857131)
3) For class 1, the split is 0 to the left and 5 to the right (zero items <= 1.72857131, five items > 1.72857131)
4) The left side Gini is (1 - ((1/1)^2 + (0/1)^2) = 0.0
5) The right side Gini is (1 - ((4/9)^2 + (5/9)^2) = 0.49382716
6) The Gini of the split is the weighted average of the left and right sides (1 * 0) + (9 * 0.49382716) = .44444444

Running this algorithm for each row gives us all the possible Gini scores for each feature:








